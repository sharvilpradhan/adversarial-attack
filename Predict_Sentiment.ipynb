{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Pred_Sent_Final.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_k4YUNfJag1",
        "colab_type": "code",
        "outputId": "a5b02166-f1c7-4864-cf94-9ff3391fab23",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(1001)\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#import tensorflow as tf\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\spradha5\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFZnS6lqJag7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import data_utils\n",
        "import glove_utils\n",
        "import display_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Vgf1c-Jag9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWhyhH5AJag_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE  = 50000\n",
        "with open('dataset_%d.pkl' %VOCAB_SIZE, 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "    \n",
        "doc_len = [len(dataset.test_seqs2[i]) for i in \n",
        "           range(len(dataset.test_seqs2))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54yPmfk0JahB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist_mat = np.load('dist_counter_%d.npy' %VOCAB_SIZE)\n",
        "# Prevent returning 0 as most similar word because it is not part of the dictionary\n",
        "dist_mat[0,:] = 100000\n",
        "dist_mat[:,0] = 100000\n",
        "\n",
        "skip_list = np.load('missed_embeddings_counter_%d.npy' %VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtn_MzcdJahE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 250\n",
        "train_x = pad_sequences(dataset.train_seqs2, maxlen=max_len, padding='post')\n",
        "train_y = np.array(dataset.train_y)\n",
        "test_x = pad_sequences(dataset.test_seqs2, maxlen=max_len, padding='post')\n",
        "test_y = np.array(dataset.test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBtr6FioJahG",
        "colab_type": "code",
        "outputId": "13dec292-13c8-478f-9753-c5df401d4433",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "if tf.get_default_session():\n",
        "    sess.close()\n",
        "sess = tf.Session()\n",
        "batch_size = 1\n",
        "lstm_size = 128\n",
        "#max_len =  100\n",
        "\n",
        "with tf.variable_scope('imdb', reuse=False):\n",
        "    model = models.SentimentModel(batch_size=batch_size,\n",
        "                           lstm_size = lstm_size,\n",
        "                           max_len = max_len,\n",
        "                           embeddings_dim=300, vocab_size=dist_mat.shape[1],is_train = False)\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(sess, 'imdb_model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\spradha5\\aldaproj\\models.py:30: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From C:\\Users\\spradha5\\aldaproj\\models.py:38: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From C:\\Users\\spradha5\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From C:\\Users\\spradha5\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From C:\\Users\\spradha5\\aldaproj\\models.py:56: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from imdb_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_VRwQ3wJahI",
        "colab_type": "code",
        "outputId": "4f94a4f0-f222-4b36-b583-f40d8c0c05ad",
        "colab": {}
      },
      "source": [
        "visual_idx = np.random.choice(25000)\n",
        "i=24997\n",
        "display_utils.visualize_attack(sess, model, dataset, test_x[i], test_x[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Prediction = Negative. (Confidence = 100.00) \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "using the lies of the title in order to bring about all manner of small twists invariably designed to surprise the characters more than the audience br br it's really rather messy though fellowes doesn't seem interested presenting the thriller elements in a fashion that will keep us seat edged rather his focus is on the moral predicaments themselves br br the dialogue is inconsistent stagey here vernacular there and with the constant surprise of realism undone by the occasional clich√© though there is no over the locations so that the actors can get on with existing in their space the dreadful score can't create a further dimension and often works against the emotional momentum of given set pieces there's also a very prosaic dare i say it british feel to the filming i didn't want to see a document of two successful middle class people caught in an extraordinary situation i wanted to see some sort of artful recounting of the story br br finally it is in fact the story which lets the rest down just as the elements of suspense are rather flat so the story is an sum of subplots of different shapes and sizes woven as a vehicle for character examination wilkinson and watson support this meta essay with good performances and john ebullient colleague simon to wilkinson is a welcome foil for much of the brow br br i'm disappointed not that it's bad but that it could have been much better 3 10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "---------  After attack -------------\n",
            "New Prediction = Negative. (Confidence = 100.00) \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "using the lies of the title in order to bring about all manner of small twists invariably designed to surprise the characters more than the audience br br it's really rather messy though fellowes doesn't seem interested presenting the thriller elements in a fashion that will keep us seat edged rather his focus is on the moral predicaments themselves br br the dialogue is inconsistent stagey here vernacular there and with the constant surprise of realism undone by the occasional clich√© though there is no over the locations so that the actors can get on with existing in their space the dreadful score can't create a further dimension and often works against the emotional momentum of given set pieces there's also a very prosaic dare i say it british feel to the filming i didn't want to see a document of two successful middle class people caught in an extraordinary situation i wanted to see some sort of artful recounting of the story br br finally it is in fact the story which lets the rest down just as the elements of suspense are rather flat so the story is an sum of subplots of different shapes and sizes woven as a vehicle for character examination wilkinson and watson support this meta essay with good performances and john ebullient colleague simon to wilkinson is a welcome foil for much of the brow br br i'm disappointed not that it's bad but that it could have been much better 3 10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKm50Iy1JahK",
        "colab_type": "code",
        "outputId": "1439ac0c-8c74-4e83-8a91-67207aa83d9d",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('reviewwordfinal.csv')\n",
        "lab = df['Label']\n",
        "typ = df['type']\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kdwZO-3JahM",
        "colab_type": "code",
        "outputId": "b02f55dd-9207-4bb8-92fc-e862ca0baa3d",
        "colab": {}
      },
      "source": [
        "i=12497\n",
        "print(lab[i])\n",
        "print(lab.size)\n",
        "print(df.Review[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neg\n",
            "25000\n",
            "The basic genre is a thriller intercut with an uncomfortable menage-a-trois. Fellowes has tried to make a lot more out of this, using the lies of the title in order to bring about all manner of small twists, invariably designed to surprise the characters more than the audience.<br /><br />It's really rather messy though. Fellowes doesn't seem interested presenting the thriller elements in a fashion that will keep us seat-edged. Rather his focus is on the moral predicaments themselves.<br /><br />The dialogue is inconsistent, stagey here, vernacular there and with the constant surprise of realism undone by the occasional clich√©-landmine. Though there is no fussing over the locations so that the actors can get on with existing in their space the dreadful score can't create a further dimension and often works against the emotional momentum of given set pieces. There's also a very prosaic, dare I say it British feel to the filming. I didn't want to see a document of two successful middle class people caught in an extraordinary situation, I wanted to see some sort of artful recounting of the story.<br /><br />Finally it is, in fact, the story which lets the rest down. Just as the elements of suspense are rather flat so the story is an asymmetric sum of subplots of different shapes and sizes, woven as a vehicle for character examination. Wilkinson and Watson support this meta-essay with good performances and John Warnaby's ebullient colleague Simon to Wilkinson is a welcome foil for much of the brow-furrowing.<br /><br />I'm disappointed; not that it's bad, but that it could have been much better. 3/10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nCrRViGJahO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,lab.size):\n",
        "    if i<12500:    #negative\n",
        "        x_orig = test_x[i+12500]\n",
        "        orig_label = test_y[i+12500]\n",
        "        orig_preds =  model.predict(sess, x_orig[np.newaxis, :])[0]\n",
        "        if np.argmax(orig_preds) == 1:\n",
        "            df.loc[i,'orig_sentiment'] = 'pos'\n",
        "        else:\n",
        "            df.loc[i,'orig_sentiment'] = 'neg'\n",
        "        \n",
        "        df.loc[i,'orig_confidence'] = round(np.max(orig_preds),4)\n",
        "        \n",
        "    else:         #positive\n",
        "        x_orig = test_x[i-12500]\n",
        "        orig_label = test_y[i-12500]\n",
        "        orig_preds =  model.predict(sess, x_orig[np.newaxis, :])[0]\n",
        "        if np.argmax(orig_preds) == 1:\n",
        "            df.loc[i,'orig_sentiment'] = 'pos'\n",
        "        else:\n",
        "            df.loc[i,'orig_sentiment'] = 'neg'\n",
        "        \n",
        "        df.loc[i,'orig_confidence'] = round(np.max(orig_preds),4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsdaO2zuJahQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_pickle('origpred.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMB4LChyJahT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('originalpred.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lADhGVFJahV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('original_pred.pkl', 'wb') as f:\n",
        "    pickle.dump(df, f)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}